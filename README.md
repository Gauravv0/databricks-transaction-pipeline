# databricks-transaction-pipeline
ğŸ“Œ Project Title

Cloud-Aware Transaction Data Engineering Pipeline using PySpark & Databricks

â¸»

ğŸ“– Problem Statement

Financial systems ingest transaction data in semi-structured JSON format from external sources.
This data must be validated, standardized, deduplicated, and transformed into analytics-ready datasets while ensuring data consistency and re-run safety.

â¸»

ğŸ§± Architecture (Medallion Pattern)

Source â†’ Bronze â†’ Silver â†’ Gold
	â€¢	Source: Finance API (CoinGecko)
	â€¢	Compute: Databricks Serverless (PySpark)
	â€¢	Storage: Unity Catalog Volumes (Delta Lake)

â¸»

ğŸŸ¤ Bronze Layer (Raw)
	â€¢	Ingests raw JSON from API
	â€¢	Stores data as-is with ingestion metadata
	â€¢	No transformations applied

Purpose: Traceability and replayability

â¸»

âšª Silver Layer (Clean & Standardized)
	â€¢	Flattens nested JSON using explode
	â€¢	Standardizes timestamps
	â€¢	Applies data quality checks (nulls, invalid values)
	â€¢	Implements idempotent incremental loads using Delta MERGE
	â€¢	Handles schema evolution using Delta mergeSchema

Purpose: Clean, deduplicated, reliable data

â¸»

ğŸŸ¡ Gold Layer (Analytics-Ready)
	â€¢	Aggregates transactional data by business date
	â€¢	Computes metrics (avg, min, max, count)
	â€¢	Exposes data via SQL views for BI tools

Purpose: Consumption by analytics and reporting teams

â¸»

ğŸ” Data Reliability & Governance
	â€¢	Uses Delta Lake ACID transactions
	â€¢	Maintains full transaction history
	â€¢	Supports time travel and auditability
	â€¢	Storage governed using Unity Catalog volumes

â¸»

ğŸ› ï¸ Tech Stack
	â€¢	Python
	â€¢	PySpark
	â€¢	Databricks (Serverless)
	â€¢	Delta Lake
	â€¢	Unity Catalog
	â€¢	SQL

â¸»

âœ… Key Features
	â€¢	End-to-end medallion architecture
	â€¢	JSON flattening & schema handling
	â€¢	Idempotent pipeline design
	â€¢	Incremental processing
	â€¢	Schema evolution support
